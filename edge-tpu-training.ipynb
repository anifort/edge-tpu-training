{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32567317",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q tflite-model-maker-nightly\n",
    "! pip install -q pycocotools\n",
    "! sudo apt-get install -y libsndfile1-dev\n",
    "! curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
    "! echo \"deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\" | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list\n",
    "! sudo apt-get update\n",
    "! sudo apt-get install edgetpu-compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba6ec1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tflite_model_maker\n",
    "\n",
    "\n",
    "#from tflite_model_maker import configs\n",
    "from tflite_model_maker.config import ExportFormat\n",
    "\n",
    "\n",
    "from tflite_model_maker import model_spec\n",
    "from tflite_model_maker import object_detector\n",
    "from tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader import DataLoader as ObjectDetectorDataloader\n",
    "\n",
    "import tensorflow as tf\n",
    "assert tf.__version__.startswith('2')\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d568383",
   "metadata": {},
   "source": [
    "Here is the performance of each EfficientDet-Lite models compared to each others.\n",
    "\n",
    "| Model architecture | Size(MB)* | Latency(ms)** | Average Precision*** |\n",
    "|--------------------|-----------|---------------|----------------------|\n",
    "| EfficientDet-Lite0 | 4.4       | 37            | 25.69%               |\n",
    "| EfficientDet-Lite1 | 5.8       | 49            | 30.55%               |\n",
    "| EfficientDet-Lite2 | 7.2       | 69            | 33.97%               |\n",
    "| EfficientDet-Lite3 | 11.4      | 116           | 37.70%               |\n",
    "| EfficientDet-Lite4 | 19.5      | 158           | 39.05%               |\n",
    "\n",
    "<i> * Size of the integer quantized models. <br/>\n",
    "** Latency measured on Pixel 4 using 4 threads on CPU. <br/>\n",
    "*** Average Precision is the mAP (mean Average Precision) on the COCO 2017 validation dataset.\n",
    "</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3e6133",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = model_spec.get('efficientdet_lite0')\n",
    "#spec = model_spec.get('efficientdet_lite1')\n",
    "#spec = model_spec.get('efficientdet_lite2')\n",
    "#spec = model_spec.get('efficientdet_lite3')\n",
    "\n",
    "#spec = ModelSpec(uri='https://tfhub.dev/google/imagenet/inception_v3/feature_vector/1')\n",
    "#spec.input_image_shape = [299, 299]\n",
    "#spec.input_image_shape "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9ff3ca",
   "metadata": {},
   "source": [
    "\n",
    "The dataset is provided in CSV format:\n",
    "```\n",
    "TRAINING,gs://cloud-ml-data/img/openimage/3/2520/3916261642_0a504acd60_o.jpg,Salad,0.0,0.0954,,,0.977,0.957,,\n",
    "VALIDATION,gs://cloud-ml-data/img/openimage/3/2520/3916261642_0a504acd60_o.jpg,Seafood,0.0154,0.1538,,,1.0,0.802,,\n",
    "TEST,gs://cloud-ml-data/img/openimage/3/2520/3916261642_0a504acd60_o.jpg,Tomato,0.0,0.655,,,0.231,0.839,,\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54944282",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, validation_data, test_data = ObjectDetectorDataloader.from_csv('gs://cloud-ml-data/img/openimage/csv/salads_ml_use.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2022ab25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = object_detector.create(train_data, model_spec=spec, batch_size=8, train_whole_model=True, validation_data=validation_data, epochs = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb6bd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81640400",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.export(export_dir='.', export_format=[ExportFormat.LABEL, ExportFormat.TFLITE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f5d37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate_tflite('model.tflite', test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5450969b",
   "metadata": {},
   "source": [
    "\n",
    "The EdgeTPU has 8MB of SRAM for caching model paramaters ([more info](https://coral.ai/docs/edgetpu/compiler/#parameter-data-caching)). This means that for models that are larger than 8MB, inference time will be increased in order to transfer over model paramaters. One way to avoid this is [Model Pipelining](https://coral.ai/docs/edgetpu/pipeline/) - splitting the model into segments that can have a dedicated EdgeTPU. This can significantly improve latency.\n",
    "\n",
    "The below table can be used as a reference for the number of Edge TPUs to use - the larger models will not compile for a single TPU as the intermediate tensors can't fit in on-chip memory.\n",
    "\n",
    "| Model architecture | Minimum TPUs | Recommended TPUs\n",
    "|--------------------|-------|-------|\n",
    "| EfficientDet-Lite0 | 1     | 1     |\n",
    "| EfficientDet-Lite1 | 1     | 1     |\n",
    "| EfficientDet-Lite2 | 1     | 2     |\n",
    "| EfficientDet-Lite3 | 2     | 2     |\n",
    "| EfficientDet-Lite4 | 2     | 3    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16aacac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_TPUS =  1\n",
    "!edgetpu_compiler model.tflite --num_segments=$NUMBER_OF_TPUS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f04aa76",
   "metadata": {},
   "source": [
    "Model Exported- Conntinue for testing! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a07896",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'model.tflite'\n",
    "label_map_path = 'labels.txt'\n",
    "\n",
    "# Load the labels into a list\n",
    "with open(label_map_path, 'r') as f:\n",
    "  classes = f.read().splitlines()\n",
    "\n",
    "# Define a list of colors for visualization\n",
    "COLORS = np.random.randint(0, 255, size=(len(classes), 3), dtype=np.uint8)\n",
    "\n",
    "def preprocess_image(image_path, input_size):\n",
    "  \"\"\"Preprocess the input image to feed to the TFLite model\"\"\"\n",
    "  img = tf.io.read_file(image_path)\n",
    "  img = tf.io.decode_image(img, channels=3)\n",
    "  img = tf.image.convert_image_dtype(img, tf.uint8)\n",
    "  original_image = img\n",
    "  resized_img = tf.image.resize(img, input_size)\n",
    "  resized_img = resized_img[tf.newaxis, :]\n",
    "  return resized_img, original_image\n",
    "\n",
    "\n",
    "def set_input_tensor(interpreter, image):\n",
    "  \"\"\"Set the input tensor.\"\"\"\n",
    "  tensor_index = interpreter.get_input_details()[0]['index']\n",
    "  input_tensor = interpreter.tensor(tensor_index)()[0]\n",
    "  input_tensor[:, :] = image\n",
    "\n",
    "\n",
    "def get_output_tensor(interpreter, index):\n",
    "  \"\"\"Retur the output tensor at the given index.\"\"\"\n",
    "  output_details = interpreter.get_output_details()[index]\n",
    "  tensor = np.squeeze(interpreter.get_tensor(output_details['index']))\n",
    "  return tensor\n",
    "\n",
    "\n",
    "def detect_objects(interpreter, image, threshold):\n",
    "  \"\"\"Returns a list of detection results, each a dictionary of object info.\"\"\"\n",
    "  # Feed the input image to the model\n",
    "  set_input_tensor(interpreter, image)\n",
    "  interpreter.invoke()\n",
    "\n",
    "  # Get all outputs from the model\n",
    "  boxes = get_output_tensor(interpreter, 0)\n",
    "  classes = get_output_tensor(interpreter, 1)\n",
    "  scores = get_output_tensor(interpreter, 2)\n",
    "  count = int(get_output_tensor(interpreter, 3))\n",
    "\n",
    "  results = []\n",
    "  for i in range(count):\n",
    "    if scores[i] >= threshold:\n",
    "      result = {\n",
    "        'bounding_box': boxes[i],\n",
    "        'class_id': classes[i],\n",
    "        'score': scores[i]\n",
    "      }\n",
    "      results.append(result)\n",
    "  return results\n",
    "\n",
    "\n",
    "def run_odt_and_draw_results(image_path, interpreter, threshold=0.5):\n",
    "  \"\"\"Run object detection on the input image and draw the detection results\"\"\"\n",
    "  # Load the input shape required by the model\n",
    "  _, input_height, input_width, _ = interpreter.get_input_details()[0]['shape']\n",
    "\n",
    "  # Load the input image and preprocess it\n",
    "  preprocessed_image, original_image = preprocess_image(\n",
    "      image_path, \n",
    "      (input_height, input_width)\n",
    "    )\n",
    "\n",
    "  # Run object detection on the input image\n",
    "  results = detect_objects(interpreter, preprocessed_image, threshold=threshold)\n",
    "\n",
    "  # Plot the detection results on the input image\n",
    "  original_image_np = original_image.numpy().astype(np.uint8)\n",
    "  for obj in results:\n",
    "    # Convert the object bounding box from relative coordinates to absolute \n",
    "    # coordinates based on the original image resolution\n",
    "    ymin, xmin, ymax, xmax = obj['bounding_box']\n",
    "    xmin = int(xmin * original_image_np.shape[1])\n",
    "    xmax = int(xmax * original_image_np.shape[1])\n",
    "    ymin = int(ymin * original_image_np.shape[0])\n",
    "    ymax = int(ymax * original_image_np.shape[0])\n",
    "\n",
    "    # Find the class index of the current object\n",
    "    class_id = int(obj['class_id'])\n",
    "\n",
    "    # Draw the bounding box and label on the image\n",
    "    color = [int(c) for c in COLORS[class_id]]\n",
    "    cv2.rectangle(original_image_np, (xmin, ymin), (xmax, ymax), color, 2)\n",
    "    # Make adjustments to make the label visible for all objects\n",
    "    y = ymin - 15 if ymin - 15 > 15 else ymin + 15\n",
    "    label = \"{}: {:.0f}%\".format(classes[class_id], obj['score'] * 100)\n",
    "    cv2.putText(original_image_np, label, (xmin, y),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "  # Return the final image\n",
    "  original_uint8 = original_image_np.astype(np.uint8)\n",
    "  return original_uint8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51791c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_IMAGE_URL = \"https://storage.googleapis.com/cloud-ml-data/img/openimage/3/2520/3916261642_0a504acd60_o.jpg\" #@param {type:\"string\"}\n",
    "DETECTION_THRESHOLD = 0.3 #@param {type:\"number\"}\n",
    "\n",
    "TEMP_FILE = 'image.png'\n",
    "\n",
    "!wget -q -O $TEMP_FILE $INPUT_IMAGE_URL\n",
    "im = Image.open(TEMP_FILE)\n",
    "im.thumbnail((512, 512), Image.ANTIALIAS)\n",
    "im.save(TEMP_FILE, 'PNG')\n",
    "\n",
    "# Load the TFLite model\n",
    "interpreter = tf.lite.Interpreter(model_path)\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd2e9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference and draw detection result on the local copy of the original file\n",
    "detection_result_image = run_odt_and_draw_results(\n",
    "    TEMP_FILE, \n",
    "    interpreter, \n",
    "    threshold=DETECTION_THRESHOLD\n",
    ")\n",
    "\n",
    "# Show the detection result\n",
    "Image.fromarray(detection_result_image)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
